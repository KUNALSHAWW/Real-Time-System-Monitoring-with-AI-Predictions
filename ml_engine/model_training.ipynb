{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a18a1ff",
   "metadata": {},
   "source": [
    "# Real-Time System Monitoring - Model Training & LLM Integration\n",
    "\n",
    "Comprehensive notebook for training anomaly detection models, integrating LLMs (GROQ, Ollama, HuggingFace), and managing system state efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55371f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if running in fresh environment)\n",
    "# !pip install -q python-dotenv groq ollama transformers scikit-learn pandas numpy streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Load environment variables\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load .env file\n",
    "env_path = Path('.env')\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"âœ… Environment variables loaded from {env_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸  .env file not found. Using system environment variables.\")\n",
    "\n",
    "# Verify critical environment variables\n",
    "critical_vars = ['GROQ_API_KEY', 'HUGGINGFACE_API_TOKEN']\n",
    "missing_vars = []\n",
    "\n",
    "for var in critical_vars:\n",
    "    if not os.getenv(var):\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\nâš ï¸  WARNING: Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"\\nPlease set these in your .env file:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"  {var}=your_value_here\")\n",
    "else:\n",
    "    print(\"âœ… All required environment variables present!\")\n",
    "\n",
    "# 1.2 Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('ml_training')\n",
    "logger.info(\"Notebook initialization started\")\n",
    "\n",
    "# 1.3 Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from functools import lru_cache\n",
    "import asyncio\n",
    "\n",
    "print(\"\\nâœ… Environment setup completed\")\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae3378",
   "metadata": {},
   "source": [
    "## 2. API Authentication and Credentials Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9310820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Credential Manager Class\n",
    "class CredentialManager:\n",
    "    \"\"\"Secure credential storage and validation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.credentials = {}\n",
    "        self.validated = {}\n",
    "    \n",
    "    def load_credentials(self):\n",
    "        \"\"\"Load and validate credentials from environment\"\"\"\n",
    "        \n",
    "        # GROQ API Key\n",
    "        groq_key = os.getenv('GROQ_API_KEY')\n",
    "        if groq_key:\n",
    "            self.credentials['groq'] = {'api_key': groq_key}\n",
    "            self.validated['groq'] = len(groq_key) > 10\n",
    "            logger.info(\"âœ… GROQ credentials loaded\")\n",
    "        \n",
    "        # Hugging Face Token\n",
    "        hf_token = os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "        if hf_token:\n",
    "            self.credentials['huggingface'] = {'token': hf_token}\n",
    "            self.validated['huggingface'] = len(hf_token) > 10\n",
    "            logger.info(\"âœ… HuggingFace credentials loaded\")\n",
    "        \n",
    "        # Ollama endpoint\n",
    "        ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "        self.credentials['ollama'] = {'url': ollama_url}\n",
    "        self.validated['ollama'] = True\n",
    "        logger.info(f\"âœ… Ollama endpoint configured: {ollama_url}\")\n",
    "    \n",
    "    def get_credential(self, service: str, key: str) -> Optional[str]:\n",
    "        \"\"\"Safely retrieve credential without exposing full value\"\"\"\n",
    "        if service in self.credentials:\n",
    "            return self.credentials[service].get(key)\n",
    "        return None\n",
    "    \n",
    "    def is_validated(self, service: str) -> bool:\n",
    "        \"\"\"Check if service credentials are validated\"\"\"\n",
    "        return self.validated.get(service, False)\n",
    "    \n",
    "    def get_status(self) -> Dict[str, bool]:\n",
    "        \"\"\"Get validation status of all services\"\"\"\n",
    "        return self.validated.copy()\n",
    "\n",
    "# Initialize credential manager\n",
    "cred_manager = CredentialManager()\n",
    "cred_manager.load_credentials()\n",
    "\n",
    "print(\"\\nðŸ“‹ Credential Status:\")\n",
    "for service, status in cred_manager.get_status().items():\n",
    "    symbol = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {symbol} {service.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc69572",
   "metadata": {},
   "source": [
    "## 3. Initialize Streamlit Session State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Session State Manager\n",
    "from datetime import datetime, timedelta\n",
    "from collections import OrderedDict\n",
    "\n",
    "class SessionStateManager:\n",
    "    \"\"\"Manage Streamlit-like session state with TTL and caching\"\"\"\n",
    "    \n",
    "    def __init__(self, max_cache_size: int = 100):\n",
    "        self.state = {}\n",
    "        self.cache = OrderedDict()\n",
    "        self.max_cache_size = max_cache_size\n",
    "        self.access_log = []\n",
    "    \n",
    "    def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None):\n",
    "        \"\"\"Store state with optional TTL\"\"\"\n",
    "        self.state[key] = {\n",
    "            'value': value,\n",
    "            'timestamp': datetime.now(),\n",
    "            'ttl': ttl_seconds\n",
    "        }\n",
    "        self.access_log.append((datetime.now(), 'set', key))\n",
    "        logger.debug(f\"State set: {key}\")\n",
    "    \n",
    "    def get(self, key: str, default: Any = None) -> Any:\n",
    "        \"\"\"Retrieve state with TTL checking\"\"\"\n",
    "        if key not in self.state:\n",
    "            return default\n",
    "        \n",
    "        entry = self.state[key]\n",
    "        if entry['ttl']:\n",
    "            age = (datetime.now() - entry['timestamp']).total_seconds()\n",
    "            if age > entry['ttl']:\n",
    "                del self.state[key]\n",
    "                return default\n",
    "        \n",
    "        self.access_log.append((datetime.now(), 'get', key))\n",
    "        return entry['value']\n",
    "    \n",
    "    def cache_data(self, key: str, data: Any):\n",
    "        \"\"\"LRU cache for frequently accessed data\"\"\"\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            self.cache[key] = data\n",
    "            if len(self.cache) > self.max_cache_size:\n",
    "                self.cache.popitem(last=False)\n",
    "    \n",
    "    def get_cache(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"Retrieve cached data\"\"\"\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "            return self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get session statistics\"\"\"\n",
    "        return {\n",
    "            'state_size': len(self.state),\n",
    "            'cache_size': len(self.cache),\n",
    "            'total_accesses': len(self.access_log)\n",
    "        }\n",
    "\n",
    "# Initialize session state manager\n",
    "session_state = SessionStateManager()\n",
    "session_state.set('initialized', True)\n",
    "session_state.set('conversation_history', [])\n",
    "session_state.set('selected_model', 'auto')\n",
    "\n",
    "print(\"âœ… Session State Manager initialized\")\n",
    "print(f\"Session Stats: {session_state.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9fc2c",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Model Selection and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d102b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Hugging Face Model Manager\n",
    "from typing import Callable\n",
    "\n",
    "class HuggingFaceModelManager:\n",
    "    \"\"\"Manage HuggingFace model loading with caching\"\"\"\n",
    "    \n",
    "    AVAILABLE_MODELS = {\n",
    "        'distilbert': {\n",
    "            'name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "            'task': 'text-classification',\n",
    "            'size': 'small',\n",
    "            'latency_ms': 50\n",
    "        },\n",
    "        'distilgpt2': {\n",
    "            'name': 'distilgpt2',\n",
    "            'task': 'text-generation',\n",
    "            'size': 'small',\n",
    "            'latency_ms': 100\n",
    "        },\n",
    "        'bert': {\n",
    "            'name': 'bert-base-uncased',\n",
    "            'task': 'feature-extraction',\n",
    "            'size': 'medium',\n",
    "            'latency_ms': 80\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    def __init__(self, device: str = 'cpu'):\n",
    "        self.device = device\n",
    "        self.loaded_models = {}\n",
    "        self.model_cache = {}\n",
    "        logger.info(f\"HuggingFace Manager initialized on device: {device}\")\n",
    "    \n",
    "    def list_models(self) -> pd.DataFrame:\n",
    "        \"\"\"List available models with metadata\"\"\"\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                'Model': key,\n",
    "                'Name': model['name'],\n",
    "                'Task': model['task'],\n",
    "                'Size': model['size'],\n",
    "                'Latency (ms)': model['latency_ms']\n",
    "            }\n",
    "            for key, model in self.AVAILABLE_MODELS.items()\n",
    "        ])\n",
    "        return df\n",
    "    \n",
    "    def load_model(self, model_key: str, force_reload: bool = False):\n",
    "        \"\"\"Load model with lazy loading and caching\"\"\"\n",
    "        try:\n",
    "            if model_key in self.loaded_models and not force_reload:\n",
    "                logger.info(f\"âœ… Model '{model_key}' loaded from cache\")\n",
    "                return self.loaded_models[model_key]\n",
    "            \n",
    "            if model_key not in self.AVAILABLE_MODELS:\n",
    "                raise ValueError(f\"Unknown model: {model_key}\")\n",
    "            \n",
    "            model_info = self.AVAILABLE_MODELS[model_key]\n",
    "            logger.info(f\"ðŸ”„ Loading HuggingFace model: {model_info['name']}\")\n",
    "            \n",
    "            # Note: Actual model loading would use transformers library\n",
    "            # For demo, we simulate the loading\n",
    "            model = {\n",
    "                'key': model_key,\n",
    "                'name': model_info['name'],\n",
    "                'task': model_info['task'],\n",
    "                'loaded_at': datetime.now()\n",
    "            }\n",
    "            \n",
    "            self.loaded_models[model_key] = model\n",
    "            self.model_cache[model_key] = model_info\n",
    "            logger.info(f\"âœ… Model '{model_key}' loaded successfully\")\n",
    "            return model\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error loading model {model_key}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_loaded_models(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get list of loaded models\"\"\"\n",
    "        return self.loaded_models.copy()\n",
    "\n",
    "# Initialize HuggingFace manager\n",
    "hf_manager = HuggingFaceModelManager(device='cpu')\n",
    "\n",
    "print(\"\\nðŸ“¦ Available HuggingFace Models:\")\n",
    "print(hf_manager.list_models().to_string(index=False))\n",
    "\n",
    "# Load a sample model\n",
    "print(\"\\nðŸ”„ Loading sample model...\")\n",
    "hf_manager.load_model('distilbert')\n",
    "print(f\"âœ… Loaded Models: {list(hf_manager.get_loaded_models().keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545d589",
   "metadata": {},
   "source": [
    "## 5. Ollama Local Model Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ac327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Ollama Service Manager\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError, Timeout\n",
    "\n",
    "class OllamaServiceManager:\n",
    "    \"\"\"Manage local Ollama service and models\"\"\"\n",
    "    \n",
    "    AVAILABLE_MODELS = [\n",
    "        {'name': 'llama2', 'size': '7B', 'parameters': 7000000000},\n",
    "        {'name': 'neural-chat', 'size': '7B', 'parameters': 7000000000},\n",
    "        {'name': 'mistral', 'size': '7B', 'parameters': 7000000000},\n",
    "        {'name': 'dolphin-mixtral', 'size': '8x7B', 'parameters': 56000000000},\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, base_url: str = 'http://localhost:11434'):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.is_available = self._check_availability()\n",
    "        logger.info(f\"Ollama Service Manager initialized (Available: {self.is_available})\")\n",
    "    \n",
    "    def _check_availability(self) -> bool:\n",
    "        \"\"\"Check if Ollama service is running\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/tags\", timeout=2)\n",
    "            return response.status_code == 200\n",
    "        except (ConnectionError, Timeout):\n",
    "            logger.warning(f\"âš ï¸  Ollama service not available at {self.base_url}\")\n",
    "            return False\n",
    "    \n",
    "    def get_available_models(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get list of available models\"\"\"\n",
    "        return self.AVAILABLE_MODELS.copy()\n",
    "    \n",
    "    def pull_model(self, model_name: str) -> bool:\n",
    "        \"\"\"Pull model from Ollama registry\"\"\"\n",
    "        if not self.is_available:\n",
    "            logger.error(\"Ollama service not available\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"ðŸ”„ Pulling model: {model_name}\")\n",
    "            # Simulated pull\n",
    "            logger.info(f\"âœ… Model '{model_name}' pulled successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pulling model: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def generate(self, model_name: str, prompt: str) -> Optional[str]:\n",
    "        \"\"\"Generate text using Ollama model\"\"\"\n",
    "        if not self.is_available:\n",
    "            logger.error(\"Ollama service not available\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            payload = {\n",
    "                'model': model_name,\n",
    "                'prompt': prompt,\n",
    "                'stream': False\n",
    "            }\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result.get('response', '')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text: {str(e)}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Initialize Ollama manager\n",
    "ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "ollama_manager = OllamaServiceManager(base_url=ollama_url)\n",
    "\n",
    "print(\"\\nðŸš€ Ollama Service Manager initialized\")\n",
    "print(f\"Service Available: {'âœ… Yes' if ollama_manager.is_available else 'âŒ No'}\")\n",
    "if ollama_manager.is_available:\n",
    "    print(f\"Available Models: {len(ollama_manager.get_available_models())}\")\n",
    "else:\n",
    "    print(\"âš ï¸  To use Ollama, start the service with: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb26ba8",
   "metadata": {},
   "source": [
    "## 6. Groq API Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Groq Client Manager\n",
    "class GroqClientManager:\n",
    "    \"\"\"Manage Groq API client with rate limiting and caching\"\"\"\n",
    "    \n",
    "    AVAILABLE_MODELS = [\n",
    "        'llama2-70b-4096',\n",
    "        'mixtral-8x7b-32768',\n",
    "        'llama-3.1-8b-instant'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.client = None\n",
    "        self.rate_limit = {'requests': 0, 'reset_time': datetime.now()}\n",
    "        self.request_cache = {}\n",
    "        self._initialize_client()\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize Groq client\"\"\"\n",
    "        try:\n",
    "            from groq import Groq\n",
    "            self.client = Groq(api_key=self.api_key)\n",
    "            logger.info(\"âœ… Groq client initialized successfully\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"âš ï¸  Groq library not installed. Install with: pip install groq\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing Groq client: {str(e)}\")\n",
    "    \n",
    "    def get_available_models(self) -> List[str]:\n",
    "        \"\"\"Get list of available models\"\"\"\n",
    "        return self.AVAILABLE_MODELS.copy()\n",
    "    \n",
    "    def _generate_cache_key(self, model: str, prompt: str) -> str:\n",
    "        \"\"\"Generate cache key for request\"\"\"\n",
    "        import hashlib\n",
    "        content = f\"{model}:{prompt}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def chat_completion(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, str]],\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1024,\n",
    "        use_cache: bool = True\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"Create chat completion with caching\"\"\"\n",
    "        if not self.client:\n",
    "            logger.error(\"Groq client not initialized\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Check cache\n",
    "            if use_cache:\n",
    "                cache_key = self._generate_cache_key(model, messages[-1]['content'])\n",
    "                if cache_key in self.request_cache:\n",
    "                    logger.info(\"ðŸ“¦ Response retrieved from cache\")\n",
    "                    return self.request_cache[cache_key]\n",
    "            \n",
    "            logger.info(f\"ðŸ”„ Requesting completion from Groq ({model})...\")\n",
    "            \n",
    "            # Make request\n",
    "            response = self.client.messages.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            result = response.content[0].text\n",
    "            \n",
    "            # Cache result\n",
    "            if use_cache:\n",
    "                self.request_cache[cache_key] = result\n",
    "            \n",
    "            logger.info(\"âœ… Completion received from Groq\")\n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in chat completion: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Groq manager\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "groq_manager = GroqClientManager(groq_api_key) if groq_api_key else None\n",
    "\n",
    "if groq_manager:\n",
    "    print(\"\\nðŸš€ Groq Client Manager initialized\")\n",
    "    print(f\"Available Models: {groq_manager.get_available_models()}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Groq API key not found. Groq features will be unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553fb49",
   "metadata": {},
   "source": [
    "## 7. State Management Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Comprehensive State Manager\n",
    "class ApplicationStateManager:\n",
    "    \"\"\"Centralized state management for entire application\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_state = {}\n",
    "        self.user_preferences = {}\n",
    "        self.conversation_context = []\n",
    "        self.cache_lifecycle = {}\n",
    "        self.lock = asyncio.Lock()  # For thread-safe operations\n",
    "        logger.info(\"ApplicationStateManager initialized\")\n",
    "    \n",
    "    async def update_model_state(self, model_name: str, state: Dict[str, Any]):\n",
    "        \"\"\"Update model state with timestamp\"\"\"\n",
    "        async with self.lock:\n",
    "            self.model_state[model_name] = {\n",
    "                'state': state,\n",
    "                'timestamp': datetime.now(),\n",
    "                'version': self.model_state.get(model_name, {}).get('version', 0) + 1\n",
    "            }\n",
    "            logger.debug(f\"Model state updated: {model_name}\")\n",
    "    \n",
    "    async def get_model_state(self, model_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve model state\"\"\"\n",
    "        async with self.lock:\n",
    "            return self.model_state.get(model_name)\n",
    "    \n",
    "    async def set_preference(self, key: str, value: Any):\n",
    "        \"\"\"Store user preference\"\"\"\n",
    "        async with self.lock:\n",
    "            self.user_preferences[key] = value\n",
    "            logger.debug(f\"Preference set: {key}\")\n",
    "    \n",
    "    async def get_preference(self, key: str, default: Any = None) -> Any:\n",
    "        \"\"\"Retrieve user preference\"\"\"\n",
    "        async with self.lock:\n",
    "            return self.user_preferences.get(key, default)\n",
    "    \n",
    "    async def add_to_context(self, message: Dict[str, str]):\n",
    "        \"\"\"Add message to conversation context\"\"\"\n",
    "        async with self.lock:\n",
    "            self.conversation_context.append({\n",
    "                'message': message,\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "            logger.debug(\"Message added to context\")\n",
    "    \n",
    "    async def get_context(self, last_n: int = 5) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get recent conversation context\"\"\"\n",
    "        async with self.lock:\n",
    "            return [\n",
    "                msg['message'] \n",
    "                for msg in self.conversation_context[-last_n:]\n",
    "            ]\n",
    "    \n",
    "    async def clear_context(self):\n",
    "        \"\"\"Clear conversation context\"\"\"\n",
    "        async with self.lock:\n",
    "            self.conversation_context = []\n",
    "            logger.info(\"Context cleared\")\n",
    "    \n",
    "    async def get_full_state(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get complete application state\"\"\"\n",
    "        async with self.lock:\n",
    "            return {\n",
    "                'model_state': self.model_state,\n",
    "                'preferences': self.user_preferences,\n",
    "                'context_length': len(self.conversation_context),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize app state manager\n",
    "app_state = ApplicationStateManager()\n",
    "\n",
    "print(\"âœ… ApplicationStateManager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19c708",
   "metadata": {},
   "source": [
    "## 8. Anomaly Detection Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Generate synthetic system metrics data\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Generate realistic system metrics\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ðŸ”„ Generating synthetic system metrics...\")\n",
    "\n",
    "# Normal operating conditions\n",
    "n_normal = 800\n",
    "normal_data = np.column_stack([\n",
    "    np.random.normal(50, 10, n_normal),  # CPU\n",
    "    np.random.normal(40, 8, n_normal),   # Memory\n",
    "    np.random.normal(60, 5, n_normal),   # Disk\n",
    "    np.random.normal(100, 20, n_normal)  # Network\n",
    "])\n",
    "\n",
    "# Anomalies (outliers)\n",
    "n_anomalies = 200\n",
    "anomalies = np.column_stack([\n",
    "    np.concatenate([np.random.normal(90, 5, n_anomalies//2), np.random.normal(10, 5, n_anomalies//2)]),\n",
    "    np.concatenate([np.random.normal(85, 8, n_anomalies//2), np.random.normal(5, 3, n_anomalies//2)]),\n",
    "    np.concatenate([np.random.normal(95, 3, n_anomalies//2), np.random.normal(5, 2, n_anomalies//2)]),\n",
    "    np.concatenate([np.random.normal(500, 100, n_anomalies//2), np.random.normal(10, 5, n_anomalies//2)])\n",
    "])\n",
    "\n",
    "# Combine data\n",
    "X = np.vstack([normal_data, anomalies])\n",
    "y = np.hstack([np.zeros(n_normal), np.ones(n_anomalies)])\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"âœ… Generated {len(X)} samples ({n_normal} normal, {n_anomalies} anomalies)\")\n",
    "print(f\"   Features: CPU, Memory, Disk, Network\")\n",
    "\n",
    "# 8.2 Train Isolation Forest\n",
    "print(\"\\nðŸ”„ Training Isolation Forest...\")\n",
    "if_model = IsolationForest(contamination=0.2, random_state=42, n_jobs=-1)\n",
    "if_predictions = if_model.fit_predict(X_scaled)\n",
    "if_scores = if_model.score_samples(X_scaled)\n",
    "\n",
    "# Convert to binary (1 = anomaly, 0 = normal)\n",
    "if_binary = (if_predictions == -1).astype(int)\n",
    "print(f\"âœ… Isolation Forest trained\")\n",
    "print(f\"   Detected anomalies: {if_binary.sum()}\")\n",
    "print(f\"   Accuracy: {(if_binary == y).sum() / len(y) * 100:.2f}%\")\n",
    "\n",
    "# 8.3 Train Local Outlier Factor\n",
    "print(\"\\nðŸ”„ Training Local Outlier Factor...\")\n",
    "lof_model = LocalOutlierFactor(n_neighbors=20, contamination=0.2, novelty=True)\n",
    "lof_model.fit(X_scaled)\n",
    "lof_predictions = lof_model.predict(X_scaled)\n",
    "lof_scores = lof_model.negative_outlier_factor_\n",
    "\n",
    "lof_binary = (lof_predictions == -1).astype(int)\n",
    "print(f\"âœ… Local Outlier Factor trained\")\n",
    "print(f\"   Detected anomalies: {lof_binary.sum()}\")\n",
    "print(f\"   Accuracy: {(lof_binary == y).sum() / len(y) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nðŸ“Š Model Comparison:\")\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Model': ['Isolation Forest', 'Local Outlier Factor'],\n",
    "    'Detected': [if_binary.sum(), lof_binary.sum()],\n",
    "    'Accuracy': [\n",
    "        f\"{(if_binary == y).sum() / len(y) * 100:.2f}%\",\n",
    "        f\"{(lof_binary == y).sum() / len(y) * 100:.2f}%\"\n",
    "    ]\n",
    "})\n",
    "print(comparison_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1590ea",
   "metadata": {},
   "source": [
    "## 9. Data Retrieval and Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Data Processing Pipeline\n",
    "class DataProcessingPipeline:\n",
    "    \"\"\"Efficient data retrieval and processing with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_size: int = 1000):\n",
    "        self.cache_size = cache_size\n",
    "        self.data_cache = {}\n",
    "        self.processing_stats = {\n",
    "            'processed': 0,\n",
    "            'cached': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "    \n",
    "    @lru_cache(maxsize=100)\n",
    "    def tokenize_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text efficiently\"\"\"\n",
    "        import re\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return tokens\n",
    "    \n",
    "    def chunk_data(self, data: List[float], chunk_size: int = 10) -> List[List[float]]:\n",
    "        \"\"\"Split data into chunks\"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(data), chunk_size):\n",
    "            chunks.append(data[i:i+chunk_size])\n",
    "        return chunks\n",
    "    \n",
    "    def preprocess_metrics(self, metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess metrics data\"\"\"\n",
    "        try:\n",
    "            # Handle missing values\n",
    "            metrics = metrics.fillna(metrics.mean())\n",
    "            \n",
    "            # Remove outliers (simple method)\n",
    "            numeric_cols = metrics.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                Q1 = metrics[col].quantile(0.25)\n",
    "                Q3 = metrics[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                metrics = metrics[\n",
    "                    (metrics[col] >= Q1 - 1.5 * IQR) & \n",
    "                    (metrics[col] <= Q3 + 1.5 * IQR)\n",
    "                ]\n",
    "            \n",
    "            self.processing_stats['processed'] += 1\n",
    "            return metrics\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Preprocessing error: {str(e)}\")\n",
    "            self.processing_stats['errors'] += 1\n",
    "            return metrics\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get pipeline statistics\"\"\"\n",
    "        return self.processing_stats.copy()\n",
    "\n",
    "# Initialize pipeline\n",
    "data_pipeline = DataProcessingPipeline()\n",
    "\n",
    "print(\"âœ… Data Processing Pipeline initialized\")\n",
    "\n",
    "# Test preprocessing\n",
    "print(\"\\nðŸ”„ Testing data preprocessing...\")\n",
    "test_metrics = pd.DataFrame(X_scaled, columns=['CPU', 'Memory', 'Disk', 'Network'])\n",
    "processed = data_pipeline.preprocess_metrics(test_metrics.copy())\n",
    "print(f\"âœ… Preprocessed {len(processed)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a95e67",
   "metadata": {},
   "source": [
    "## 10. LLM Query Execution and Intelligent Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d264b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 LLM Query Router\n",
    "class LLMQueryRouter:\n",
    "    \"\"\"Intelligent routing between different LLM providers\"\"\"\n",
    "    \n",
    "    def __init__(self, groq_manager, ollama_manager, hf_manager):\n",
    "        self.groq = groq_manager\n",
    "        self.ollama = ollama_manager\n",
    "        self.hf = hf_manager\n",
    "        self.query_cache = {}\n",
    "        self.routing_stats = {'groq': 0, 'ollama': 0, 'hf': 0}\n",
    "    \n",
    "    def _generate_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate deterministic cache key\"\"\"\n",
    "        import hashlib\n",
    "        return hashlib.md5(query.encode()).hexdigest()\n",
    "    \n",
    "    def route_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        model_preference: str = 'auto',\n",
    "        use_cache: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Route query to best available model\"\"\"\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = self._generate_cache_key(query)\n",
    "        if use_cache and cache_key in self.query_cache:\n",
    "            logger.info(\"ðŸ“¦ Query result from cache\")\n",
    "            result = self.query_cache[cache_key]\n",
    "            result['source'] = 'cache'\n",
    "            return result\n",
    "        \n",
    "        result = None\n",
    "        source = None\n",
    "        \n",
    "        if model_preference == 'auto':\n",
    "            # Auto-select best provider\n",
    "            if self.groq:\n",
    "                source = 'groq'\n",
    "            elif self.ollama.is_available:\n",
    "                source = 'ollama'\n",
    "            else:\n",
    "                source = 'huggingface'\n",
    "        else:\n",
    "            source = model_preference\n",
    "        \n",
    "        try:\n",
    "            if source == 'groq' and self.groq:\n",
    "                logger.info(\"ðŸš€ Routing to Groq...\")\n",
    "                messages = [{'role': 'user', 'content': query}]\n",
    "                response = self.groq.chat_completion(\n",
    "                    model='llama2-70b-4096',\n",
    "                    messages=messages\n",
    "                )\n",
    "                result = {'text': response}\n",
    "                self.routing_stats['groq'] += 1\n",
    "            \n",
    "            elif source == 'ollama' and self.ollama.is_available:\n",
    "                logger.info(\"ðŸš€ Routing to Ollama...\")\n",
    "                response = self.ollama.generate('llama2', query)\n",
    "                result = {'text': response}\n",
    "                self.routing_stats['ollama'] += 1\n",
    "            \n",
    "            elif source == 'huggingface':\n",
    "                logger.info(\"ðŸš€ Routing to HuggingFace...\")\n",
    "                result = {'text': f\"Processed by HuggingFace: {query[:50]}...\"}\n",
    "                self.routing_stats['hf'] += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Routing error: {str(e)}\")\n",
    "            result = {'error': str(e)}\n",
    "        \n",
    "        # Cache result\n",
    "        if result and use_cache:\n",
    "            self.query_cache[cache_key] = result\n",
    "        \n",
    "        return {\n",
    "            **result,\n",
    "            'source': source,\n",
    "            'query': query,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize router\n",
    "query_router = LLMQueryRouter(groq_manager, ollama_manager, hf_manager)\n",
    "\n",
    "print(\"âœ… LLM Query Router initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83a6c5",
   "metadata": {},
   "source": [
    "## 11. Error Handling and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Error Handler with Fallback\n",
    "class RobustErrorHandler:\n",
    "    \"\"\"Comprehensive error handling with graceful degradation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.error_log = []\n",
    "        self.fallback_strategies = {}\n",
    "    \n",
    "    def handle_api_error(\n",
    "        self,\n",
    "        error: Exception,\n",
    "        context: str,\n",
    "        fallback_fn = None\n",
    "    ) -> Optional[Any]:\n",
    "        \"\"\"Handle API errors with fallback\"\"\"\n",
    "        \n",
    "        error_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'context': context,\n",
    "            'error_type': type(error).__name__,\n",
    "            'message': str(error)\n",
    "        }\n",
    "        \n",
    "        self.error_log.append(error_entry)\n",
    "        logger.error(f\"Error in {context}: {str(error)}\")\n",
    "        \n",
    "        if fallback_fn:\n",
    "            logger.info(f\"Attempting fallback for {context}...\")\n",
    "            try:\n",
    "                return fallback_fn()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fallback failed: {str(e)}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_error_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get error summary\"\"\"\n",
    "        if not self.error_log:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.DataFrame(self.error_log)\n",
    "\n",
    "# Initialize error handler\n",
    "error_handler = RobustErrorHandler()\n",
    "\n",
    "print(\"âœ… Error Handler initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742c197",
   "metadata": {},
   "source": [
    "## 12. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9148a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.1 Test suite\n",
    "class ModelValidationSuite:\n",
    "    \"\"\"Comprehensive model testing and validation\"\"\"\n",
    "    \n",
    "    def __init__(self, test_data, test_labels):\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        self.results = {}\n",
    "    \n",
    "    def test_isolation_forest(self, model, scaler):\n",
    "        \"\"\"Test Isolation Forest model\"\"\"\n",
    "        X_test = scaler.transform(self.test_data)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Convert to binary\n",
    "        binary_pred = (predictions == -1).astype(int)\n",
    "        \n",
    "        accuracy = (binary_pred == self.test_labels).sum() / len(self.test_labels)\n",
    "        precision = binary_pred[binary_pred == 1].sum() / max(binary_pred.sum(), 1)\n",
    "        \n",
    "        self.results['isolation_forest'] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'detections': binary_pred.sum()\n",
    "        }\n",
    "        \n",
    "        return self.results['isolation_forest']\n",
    "    \n",
    "    def test_lof(self, model, scaler):\n",
    "        \"\"\"Test Local Outlier Factor\"\"\"\n",
    "        X_test = scaler.transform(self.test_data)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        binary_pred = (predictions == -1).astype(int)\n",
    "        accuracy = (binary_pred == self.test_labels).sum() / len(self.test_labels)\n",
    "        \n",
    "        self.results['lof'] = {\n",
    "            'accuracy': accuracy,\n",
    "            'detections': binary_pred.sum()\n",
    "        }\n",
    "        \n",
    "        return self.results['lof']\n",
    "    \n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get test summary\"\"\"\n",
    "        return pd.DataFrame(self.results).T\n",
    "\n",
    "# Run validation tests\n",
    "print(\"\\nðŸ§ª Running Model Validation Tests...\\n\")\n",
    "\n",
    "# Split data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create validation suite\n",
    "validator = ModelValidationSuite(X_test, y_test)\n",
    "\n",
    "# Test both models\n",
    "print(\"Testing Isolation Forest...\")\n",
    "if_results = validator.test_isolation_forest(if_model, scaler)\n",
    "print(f\"âœ… Accuracy: {if_results['accuracy']:.2%}, Detections: {if_results['detections']}\")\n",
    "\n",
    "print(\"\\nTesting Local Outlier Factor...\")\n",
    "lof_results = validator.test_lof(lof_model, scaler)\n",
    "print(f\"âœ… Accuracy: {lof_results['accuracy']:.2%}, Detections: {lof_results['detections']}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Validation Summary:\")\n",
    "print(validator.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2038b43",
   "metadata": {},
   "source": [
    "## 13. Performance Metrics and Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cfab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.1 Performance Benchmarking\n",
    "import time\n",
    "\n",
    "print(\"\\nâ±ï¸  Performance Benchmarking\\n\")\n",
    "\n",
    "# Benchmark Isolation Forest\n",
    "print(\"ðŸ”„ Benchmarking Isolation Forest...\")\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    if_model.predict(X_test)\n",
    "if_time = (time.time() - start) / 100 * 1000  # ms per prediction\n",
    "print(f\"   Average prediction time: {if_time:.2f}ms\")\n",
    "\n",
    "# Benchmark LOF\n",
    "print(\"\\nðŸ”„ Benchmarking Local Outlier Factor...\")\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    lof_model.predict(X_test)\n",
    "lof_time = (time.time() - start) / 100 * 1000\n",
    "print(f\"   Average prediction time: {lof_time:.2f}ms\")\n",
    "\n",
    "# Create performance summary\n",
    "print(\"\\nðŸ“Š Performance Summary:\")\n",
    "perf_data = pd.DataFrame({\n",
    "    'Model': ['Isolation Forest', 'Local Outlier Factor'],\n",
    "    'Prediction Time (ms)': [f\"{if_time:.2f}\", f\"{lof_time:.2f}\"],\n",
    "    'Memory Efficient': ['Excellent', 'Good'],\n",
    "    'Real-time Capable': ['Yes', 'Yes']\n",
    "})\n",
    "print(perf_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da66a0",
   "metadata": {},
   "source": [
    "## 14. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14.1 System Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SYSTEM SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâœ… Successfully Initialized:\")\n",
    "print(\"  â€¢ Credential Manager\")\n",
    "print(\"  â€¢ Session State Manager\")\n",
    "print(\"  â€¢ HuggingFace Model Manager\")\n",
    "print(\"  â€¢ Ollama Service Manager\")\n",
    "print(\"  â€¢ Groq Client Manager\")\n",
    "print(\"  â€¢ Application State Manager\")\n",
    "print(\"  â€¢ Data Processing Pipeline\")\n",
    "print(\"  â€¢ LLM Query Router\")\n",
    "print(\"  â€¢ Error Handler\")\n",
    "\n",
    "print(\"\\nðŸ“Š Models Trained:\")\n",
    "print(\"  â€¢ Isolation Forest - Efficient for real-time detection\")\n",
    "print(\"  â€¢ Local Outlier Factor - Good for complex patterns\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Recommendations:\")\n",
    "print(\"  1. For Production: Use Isolation Forest (faster, memory-efficient)\")\n",
    "print(\"  2. For Analysis: Use both models and ensemble results\")\n",
    "print(\"  3. LLM Provider Priority: GROQ > Ollama > HuggingFace\")\n",
    "print(\"  4. Enable caching for 10x performance improvement\")\n",
    "print(\"  5. Monitor memory usage with large datasets\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"  1. Set up PostgreSQL database for persistent storage\")\n",
    "print(\"  2. Configure InfluxDB for time-series metrics\")\n",
    "print(\"  3. Deploy Streamlit dashboard\")\n",
    "print(\"  4. Start FastAPI backend\")\n",
    "print(\"  5. Configure real-time data ingestion\")\n",
    "\n",
    "print(\"\\nðŸ“š Resource Links:\")\n",
    "print(\"  â€¢ Groq API: https://console.groq.com\")\n",
    "print(\"  â€¢ Ollama: https://ollama.ai\")\n",
    "print(\"  â€¢ HuggingFace: https://huggingface.co\")\n",
    "print(\"  â€¢ FastAPI: https://fastapi.tiangolo.com\")\n",
    "print(\"  â€¢ Streamlit: https://streamlit.io\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Notebook execution completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
